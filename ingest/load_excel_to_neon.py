#!/usr/bin/env python3
"""
Excel ‚Üí Neon Postgres Ingestion Script

Loads SupportMind__Final_Data.xlsx into Neon Postgres database.
Reads DATABASE_URL from .env file.

Usage:
    python -m ingest.load_excel_to_neon
    
    # Or with explicit path:
    python -m ingest.load_excel_to_neon --excel data/raw/SupportMind__Final_Data.xlsx
"""

import os
import re
import argparse
from pathlib import Path

import pandas as pd
from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

# Load environment variables
load_dotenv()

# Sheet ‚Üí Table mapping
SHEET_TABLE_MAPPING = {
    "Tickets": "tickets",
    "Conversations": "conversations",
    "Knowledge_Articles": "knowledge_articles",
    "Existing_Knowledge_Articles": "existing_knowledge_articles",
    "Learning_Events": "learning_events",
    "Scripts_Master": "scripts_master",
    "Placeholder_Dictionary": "placeholder_dictionary",
    "KB_Lineage": "kb_lineage",
}

# Sheets to ignore
IGNORE_SHEETS = {"README", "Questions", "QA_Evaluation_Prompt"}


def normalize_column_name(col: str) -> str:
    """Convert column name to snake_case."""
    # Replace spaces and special chars with underscore
    col = re.sub(r"[^\w]+", "_", col.strip())
    # Convert CamelCase to snake_case
    col = re.sub(r"([a-z])([A-Z])", r"\1_\2", col)
    # Lowercase and remove leading/trailing underscores
    col = col.lower().strip("_")
    return col


def load_excel(excel_path: str) -> dict[str, pd.DataFrame]:
    """Load all relevant sheets from Excel file."""
    print(f"üìÇ Loading Excel file: {excel_path}")
    xl = pd.ExcelFile(excel_path)
    
    sheets = {}
    for sheet_name in xl.sheet_names:
        if sheet_name in IGNORE_SHEETS:
            print(f"  ‚è≠Ô∏è  Skipping sheet: {sheet_name}")
            continue
        if sheet_name not in SHEET_TABLE_MAPPING:
            print(f"  ‚ö†Ô∏è  Unknown sheet (skipping): {sheet_name}")
            continue
            
        df = pd.read_excel(xl, sheet_name=sheet_name)
        # Normalize column names
        df.columns = [normalize_column_name(c) for c in df.columns]
        sheets[sheet_name] = df
        print(f"  ‚úÖ Loaded {sheet_name}: {len(df)} rows, {len(df.columns)} columns")
    
    return sheets


def apply_schema(engine) -> None:
    """Apply database schema from schema.sql."""
    schema_path = Path(__file__).parent.parent / "db" / "schema.sql"
    
    if not schema_path.exists():
        raise FileNotFoundError(f"Schema file not found: {schema_path}")
    
    print(f"üìã Applying schema from: {schema_path}")
    schema_sql = schema_path.read_text()
    
    with engine.connect() as conn:
        # Execute schema (handles DROP and CREATE)
        conn.execute(text(schema_sql))
        conn.commit()
    
    print("  ‚úÖ Schema applied successfully")


def insert_dataframe(engine, table_name: str, df: pd.DataFrame) -> int:
    """Insert DataFrame into table. Returns row count."""
    if df.empty:
        print(f"  ‚ö†Ô∏è  {table_name}: Empty dataframe, skipping")
        return 0
    
    # Convert all columns to string to avoid type issues
    df = df.astype(str)
    # Replace 'nan' strings with None
    df = df.replace({"nan": None, "NaN": None, "NaT": None})
    
    # Insert using pandas to_sql (append mode, schema already created)
    df.to_sql(
        table_name,
        engine,
        if_exists="append",
        index=False,
        method="multi",  # Batch inserts
    )
    
    return len(df)


def extract_evidence_units(engine) -> int:
    """Extract evidence units from Postgres tables for draft generation."""
    Session = sessionmaker(bind=engine)
    session = Session()
    try:
        session.execute(text("DELETE FROM evidence_units"))
        session.commit()

        count = 0

        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT ticket_number, subject, description, root_cause, resolution
                FROM tickets
            """))
            for row in result:
                ticket_id = row[0]
                for field_idx, field_name in enumerate(
                    ["Subject", "Description", "Root_Cause", "Resolution"], start=1
                ):
                    value = row[field_idx]
                    if not value or str(value).strip().lower() == "nan":
                        continue
                    text_value = str(value)
                    evidence_unit_id = f"EU-TICKET-{ticket_id}-{field_name}-0"
                    session.execute(text("""
                        INSERT INTO evidence_units (
                            evidence_unit_id,
                            source_type,
                            source_id,
                            field_name,
                            char_offset_start,
                            char_offset_end,
                            chunk_index,
                            snippet_text
                        ) VALUES (
                            :evidence_unit_id,
                            'TICKET',
                            :source_id,
                            :field_name,
                            0,
                            :char_offset_end,
                            0,
                            :snippet_text
                        )
                    """), {
                        "evidence_unit_id": evidence_unit_id,
                        "source_id": ticket_id,
                        "field_name": field_name,
                        "char_offset_end": len(text_value),
                        "snippet_text": text_value[:2000],
                    })
                    count += 1

            result = conn.execute(text("""
                SELECT conversation_id, issue_summary, transcript
                FROM conversations
            """))
            for row in result:
                conversation_id = row[0]
                for field_idx, field_name in enumerate(
                    ["Issue_Summary", "Transcript"], start=1
                ):
                    value = row[field_idx]
                    if not value or str(value).strip().lower() == "nan":
                        continue
                    text_value = str(value)
                    evidence_unit_id = f"EU-CONVERSATION-{conversation_id}-{field_name}-0"
                    session.execute(text("""
                        INSERT INTO evidence_units (
                            evidence_unit_id,
                            source_type,
                            source_id,
                            field_name,
                            char_offset_start,
                            char_offset_end,
                            chunk_index,
                            snippet_text
                        ) VALUES (
                            :evidence_unit_id,
                            'CONVERSATION',
                            :source_id,
                            :field_name,
                            0,
                            :char_offset_end,
                            0,
                            :snippet_text
                        )
                    """), {
                        "evidence_unit_id": evidence_unit_id,
                        "source_id": conversation_id,
                        "field_name": field_name,
                        "char_offset_end": len(text_value),
                        "snippet_text": text_value[:2000],
                    })
                    count += 1

        session.commit()
        return count
    finally:
        session.close()


def main():
    parser = argparse.ArgumentParser(description="Load Excel data into Neon Postgres")
    parser.add_argument(
        "--excel",
        default="data/raw/SupportMind__Final_Data.xlsx",
        help="Path to Excel file",
    )
    parser.add_argument(
        "--skip-schema",
        action="store_true",
        help="Skip schema application (use existing tables)",
    )
    args = parser.parse_args()
    
    # Get database URL
    database_url = os.getenv("DATABASE_URL")
    if not database_url:
        raise ValueError("DATABASE_URL not found in environment. Check .env file.")
    
    print("üöÄ Trust-Me-Bro Excel ‚Üí Neon Ingestion")
    print("=" * 50)
    
    # Create database engine
    engine = create_engine(database_url)
    print(f"üîå Connected to database")
    
    # Apply schema (drops and recreates tables)
    if not args.skip_schema:
        apply_schema(engine)
    else:
        print("‚è≠Ô∏è  Skipping schema application")
    
    # Load Excel sheets
    sheets = load_excel(args.excel)
    
    # Insert data into tables
    print("\nüì• Inserting data into tables...")
    total_rows = 0
    
    for sheet_name, df in sheets.items():
        table_name = SHEET_TABLE_MAPPING[sheet_name]
        try:
            rows = insert_dataframe(engine, table_name, df)
            total_rows += rows
            print(f"  ‚úÖ {table_name}: {rows} rows inserted")
        except Exception as e:
            print(f"  ‚ùå {table_name}: Error - {e}")
            raise
    
    print("\n" + "=" * 50)
    print(f"‚úÖ Ingestion complete! Total rows: {total_rows}")
    
    # Verify counts
    print("\nüìä Verification (row counts):")
    with engine.connect() as conn:
        for table_name in SHEET_TABLE_MAPPING.values():
            result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
            count = result.scalar()
            print(f"  {table_name}: {count} rows")

    print("\nüì¶ Extracting evidence units...")
    evidence_count = extract_evidence_units(engine)
    print(f"  ‚úÖ Extracted {evidence_count} evidence units")


if __name__ == "__main__":
    main()
